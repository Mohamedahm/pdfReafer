{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f94c5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 13:27:51.514 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\moham\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "#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\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import json\n",
    "from  langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "def main():\n",
    "    with open('api_key.json', 'r') as key:\n",
    "        api_data = json.load(key)\n",
    "\n",
    "    OPENAI_API_KEY = api_data['api_key']\n",
    "\n",
    "    st.set_page_config(page_title = 'Ask your pdf')\n",
    "    st.header('Ask your pdf')\n",
    "\n",
    "    pdf = st.file_uploader(\"upload your pdf\" , type = \"pdf\")\n",
    "\n",
    "    if pdf:\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        text = ''\n",
    "\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "        # split into chuncks\n",
    "        text_spilter = CharacterTextSplitter(\n",
    "\n",
    "            separator='\\n',\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "        )\n",
    "        chunks = text_spilter.split_text(text)\n",
    "\n",
    "        # create embeddins\n",
    "        embeddins = OpenAIEmbeddings(openai_api_key='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')\n",
    "        knowledge_pdf = FAISS.from_texts(chunks , embeddins)\n",
    "\n",
    "        #st.write(chunks)\n",
    "\n",
    "        user_question = st.text_input(\"Ask question about pdf\")\n",
    "\n",
    "        if user_question:\n",
    "            docs = knowledge_pdf.similarity_search(user_question)\n",
    "\n",
    "            llm = OpenAI(openai_api_key='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')\n",
    "            chain = load_qa_chain(llm , chain_type=\"stuff\")\n",
    "            responce = chain.run(input_documents = docs , question = user_question)\n",
    "\n",
    "            st.write(responce)\n",
    "\n",
    "if __name__  == '__main__':\n",
    "\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a4d6279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting langchain==0.0.154 (from -r requirments.txt (line 1))\n",
      "  Obtaining dependency information for langchain==0.0.154 from https://files.pythonhosted.org/packages/f2/62/65a7d0a2f3a3424c08735d5cb558f908660064adad3428c1bb32130a4eb4/langchain-0.0.154-py3-none-any.whl.metadata\n",
      "  Using cached langchain-0.0.154-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: PyPDF2==3.0.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from -r requirments.txt (line 2)) (3.0.1)\n",
      "Requirement already satisfied: python-dotenv==1.0.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from -r requirments.txt (line 3)) (1.0.0)\n",
      "Collecting streamlit==1.18.1 (from -r requirments.txt (line 4))\n",
      "  Obtaining dependency information for streamlit==1.18.1 from https://files.pythonhosted.org/packages/75/83/aab60f6a6d9317ff546b67803d17027c381aedfbb29b45507784ad77f889/streamlit-1.18.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached streamlit-1.18.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: faiss-cpu==1.7.4 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from -r requirments.txt (line 5)) (1.7.4)\n",
      "Collecting altair<5 (from -r requirments.txt (line 6))\n",
      "  Obtaining dependency information for altair<5 from https://files.pythonhosted.org/packages/18/62/47452306e84d4d2e67f9c559380aeb230f5e6ca84fafb428dd36b96a99ba/altair-4.2.2-py3-none-any.whl.metadata\n",
      "  Using cached altair-4.2.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from langchain==0.0.154->-r requirments.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>1.4 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from langchain==0.0.154->-r requirments.txt (line 1)) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from langchain==0.0.154->-r requirments.txt (line 1)) (3.8.5)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.154->-r requirments.txt (line 1))\n",
      "  Obtaining dependency information for dataclasses-json<0.6.0,>=0.5.7 from https://files.pythonhosted.org/packages/97/5f/e7cc90f36152810cab08b6c9c1125e8bcb9d76f8b3018d101b5f877b386c/dataclasses_json-0.5.14-py3-none-any.whl.metadata\n",
      "  Using cached dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from langchain==0.0.154->-r requirments.txt (line 1)) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from langchain==0.0.154->-r requirments.txt (line 1)) (1.24.3)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.154->-r requirments.txt (line 1))\n",
      "  Obtaining dependency information for openapi-schema-pydantic<2.0,>=1.2 from https://files.pythonhosted.org/packages/a8/e7/22abb5a10733bf8142984201aedf27d4a58f5810ebdfe9679f9876c7bf4d/openapi_schema_pydantic-1.2.4-py3-none-any.whl.metadata\n",
      "  Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from langchain==0.0.154->-r requirments.txt (line 1)) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from langchain==0.0.154->-r requirments.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from langchain==0.0.154->-r requirments.txt (line 1)) (8.2.2)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from langchain==0.0.154->-r requirments.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: blinker>=1.0.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: cachetools>=4.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (5.3.2)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (8.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (6.0.0)\n",
      "Requirement already satisfied: packaging>=14.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (23.1)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (2.0.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (9.4.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.12 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=4.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (11.0.0)\n",
      "Requirement already satisfied: pympler>=0.9 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (13.7.1)\n",
      "Requirement already satisfied: semver in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: toml in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (4.10.0)\n",
      "Requirement already satisfied: tzlocal>=1.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (5.2)\n",
      "Requirement already satisfied: validators>=0.2 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (0.28.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (3.1.43)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (0.8.1b0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (6.3.2)\n",
      "Requirement already satisfied: watchdog in c:\\users\\moham\\anaconda3\\lib\\site-packages (from streamlit==1.18.1->-r requirments.txt (line 4)) (2.1.6)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\moham\\anaconda3\\lib\\site-packages (from altair<5->-r requirments.txt (line 6)) (0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from altair<5->-r requirments.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from altair<5->-r requirments.txt (line 6)) (4.17.3)\n",
      "Requirement already satisfied: toolz in c:\\users\\moham\\anaconda3\\lib\\site-packages (from altair<5->-r requirments.txt (line 6)) (0.12.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.154->-r requirments.txt (line 1)) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.154->-r requirments.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.154->-r requirments.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.154->-r requirments.txt (line 1)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.154->-r requirments.txt (line 1)) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.154->-r requirments.txt (line 1)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.154->-r requirments.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\moham\\anaconda3\\lib\\site-packages (from click>=7.0->streamlit==1.18.1->-r requirments.txt (line 4)) (0.4.6)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.154->-r requirments.txt (line 1))\n",
      "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/38/04/37055b7013dfaaf66e3a9a51e46857cc9be151476a891b995fa70da7e139/marshmallow-3.21.1-py3-none-any.whl.metadata\n",
      "  Using cached marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.154->-r requirments.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19->streamlit==1.18.1->-r requirments.txt (line 4)) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.4->streamlit==1.18.1->-r requirments.txt (line 4)) (3.11.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<5->-r requirments.txt (line 6)) (0.18.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from pandas>=0.25->streamlit==1.18.1->-r requirments.txt (line 4)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from pandas>=0.25->streamlit==1.18.1->-r requirments.txt (line 4)) (2023.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from jinja2->altair<5->-r requirments.txt (line 6)) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from python-dateutil->streamlit==1.18.1->-r requirments.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.0.154->-r requirments.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.0.154->-r requirments.txt (line 1)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.0.154->-r requirments.txt (line 1)) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from rich>=10.11.0->streamlit==1.18.1->-r requirments.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from rich>=10.11.0->streamlit==1.18.1->-r requirments.txt (line 4)) (2.15.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>1.4->langchain==0.0.154->-r requirments.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit==1.18.1->-r requirments.txt (line 4)) (5.0.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->streamlit==1.18.1->-r requirments.txt (line 4)) (0.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.154->-r requirments.txt (line 1)) (1.0.0)\n",
      "Using cached langchain-0.0.154-py3-none-any.whl (709 kB)\n",
      "Using cached streamlit-1.18.1-py2.py3-none-any.whl (9.6 MB)\n",
      "Using cached altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "Using cached dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "Using cached marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "Installing collected packages: marshmallow, openapi-schema-pydantic, dataclasses-json, langchain, altair, streamlit\n",
      "Successfully installed altair-4.2.2 dataclasses-json-0.5.14 langchain-0.0.154 marshmallow-3.21.1 openapi-schema-pydantic-1.2.4 streamlit-1.18.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script langchain-server.exe is installed in 'C:\\Users\\moham\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script streamlit.exe is installed in 'C:\\Users\\moham\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirments.txt --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aafd1a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1465090008.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    streamlit run main.py\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "streamlit run main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "212ee7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "599060e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at aubmindlab/bert-base-arabert and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: [SEP] الطقس اليوم في القاهرة جميل ومش\n",
      "True Answer: جميل ومشمس\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained model and tokenizer for Arabic\n",
    "model_name = \"aubmindlab/bert-base-arabert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Example Arabic text for training\n",
    "context = \"الطقس اليوم في القاهرة جميل ومشمس.\"\n",
    "question = \"كيف الطقس\"\n",
    "answer = \"جميل ومشمس\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Model forward pass\n",
    "outputs = model(**inputs)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Convert logits to probabilities\n",
    "start_probs = torch.softmax(start_logits, dim=1).squeeze()\n",
    "end_probs = torch.softmax(end_logits, dim=1).squeeze()\n",
    "\n",
    "# Get the most probable answer span\n",
    "start_idx = torch.argmax(start_probs)\n",
    "end_idx = torch.argmax(end_probs)\n",
    "answer_span = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx+1]))\n",
    "\n",
    "print(\"Predicted Answer:\", answer_span)\n",
    "print(\"True Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3804311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at aubmindlab/bert-base-arabert and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ماذا يكون الطقس في القاهرة اليوم؟\n",
      "Predicted Answer: \n",
      "True Answer: جميل ومشمس\n",
      "\n",
      "Question: ما هو الوضع الجوي في القاهرة اليوم؟\n",
      "Predicted Answer: اليوم ؟\n",
      "True Answer: جميل ومشمس\n",
      "\n",
      "Question: متى تأسست الإسكندرية؟\n",
      "Predicted Answer: \n",
      "True Answer: 331 قبل الميلاد\n",
      "\n",
      "Question: ما هي سنة تأسيس الإسكندرية؟\n",
      "Predicted Answer: \n",
      "True Answer: 331 قبل الميلاد\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained model and tokenizer for Arabic\n",
    "model_name = \"aubmindlab/bert-base-arabert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Example Arabic text for training\n",
    "training_data = [\n",
    "    {\n",
    "        \"context\": \"الطقس اليوم في القاهرة جميل ومشمس.\",\n",
    "        \"questions\": [\n",
    "            \"ماذا يكون الطقس في القاهرة اليوم؟\",\n",
    "            \"ما هو الوضع الجوي في القاهرة اليوم؟\"\n",
    "        ],\n",
    "        \"answers\": [\"جميل ومشمس\", \"جميل ومشمس\"]\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"تم تأسيس الإسكندرية عام 331 قبل الميلاد.\",\n",
    "        \"questions\": [\n",
    "            \"متى تأسست الإسكندرية؟\",\n",
    "            \"ما هي سنة تأسيس الإسكندرية؟\"\n",
    "        ],\n",
    "        \"answers\": [\"331 قبل الميلاد\", \"331 قبل الميلاد\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Iterate through each training example\n",
    "for example in training_data:\n",
    "    context = example[\"context\"]\n",
    "    for i, question in enumerate(example[\"questions\"]):\n",
    "        answer = example[\"answers\"][i]\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(question, context, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        # Model forward pass\n",
    "        outputs = model(**inputs)\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "        # Convert logits to probabilities\n",
    "        start_probs = torch.softmax(start_logits, dim=1).squeeze()\n",
    "        end_probs = torch.softmax(end_logits, dim=1).squeeze()\n",
    "        # Get the most probable answer span\n",
    "        start_idx = torch.argmax(start_probs)\n",
    "        end_idx = torch.argmax(end_probs)\n",
    "        answer_span = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx+1]))\n",
    "        print(\"Question:\", question)\n",
    "        print(\"Predicted Answer:\", answer_span)\n",
    "        print(\"True Answer:\", answer)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28a6c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(page_title='Ask your pdf')\n",
    "    st.header('Ask your pdf')\n",
    "\n",
    "    pdf = st.file_uploader(\"Upload your pdf\", type=\"pdf\")\n",
    "\n",
    "    if pdf:\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        text = ''\n",
    "\n",
    "        # Extract text from PDF\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "        # Load pre-trained model and tokenizer for Arabic\n",
    "        model_name = \"aubmindlab/bert-base-arabert\"\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "        # Example questions\n",
    "        questions = [\n",
    "            \"ما هو مضمون النص؟\",\n",
    "            \"ما هي المعلومات المذكورة في النص؟\"\n",
    "        ]\n",
    "\n",
    "        # Tokenize and get answers for each question\n",
    "        for question in questions:\n",
    "            inputs = tokenizer(question, text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "            outputs = model(**inputs)\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "            start_idx = torch.argmax(start_logits)\n",
    "            end_idx = torch.argmax(end_logits)\n",
    "            answer_span = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx+1]))\n",
    "            \n",
    "            st.write(\"Question:\", question)\n",
    "            st.write(\"Predicted Answer:\", answer_span)\n",
    "            st.write(\"---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4510f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\moham\\\\PycharmProjects\\\\answerpdf'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1d59b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
